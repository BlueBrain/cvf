import argparse
import copy
import glob
import logging
import os
import subprocess
import warnings
from itertools import cycle
from pathlib import Path

import numpy as np
import yaml

from . import utils
from .config import Config
from .run_result import RunResult, Result
from .simulation import Simulation

try:
    import matplotlib.pyplot as pplt
    from matplotlib import colors
except ImportError:
    logging.warning("Matplotlib could no be found. Proceeding without plots...")


# do not import from neuron


class CompareTestResultsError(Exception):
    pass


def cvf_stdrun():
    """Cvf wrapper of run for standard test run

    "cvf_stdrun -h" for more information
    """

    parser = argparse.ArgumentParser(
        description="Channel-validation-framework standard test run."
    )
    parser.add_argument(
        "-c",
        "--config",
        default="config",
        type=str,
        help="Directory of the config files (i.e. cvf_template.yaml).",
    )
    parser.add_argument(
        "-r",
        "--run_config",
        default="run_config.yaml",
        type=str,
        help="Location of the run_config.yaml file.",
    )
    parser.add_argument(
        "-d",
        "--working_dir",
        default="tmp",
        type=str,
        help="Location where the program is going to generate results and files.",
    )
    parser.add_argument(
        "mod_dirs",
        nargs="*",
        default=["mod/cvf", "mod/local"],
        type=str,
        help="Mod dirs that are to be processed (non-recursive).",
    )

    args = parser.parse_args()

    results = run(
        config_files_dir=args.config,
        mod_dirs=args.mod_dirs,
        run_config_path=args.run_config,
        base_working_dir=args.working_dir
    )

    compare(results, output_dir=args.working_dir)

    return 0


def run(
    config_files_dir="config",
    config_protocol_generator=Config.ProtocolGenerator.SI_FIRST_PROTOCOL,
    mod_dirs=["mod/cvf", "mod/local"],
    run_config_path="run_config.yaml",
    base_working_dir="tmp",
    print_config=False,
    clear_working_dir=True,
):
    """Main cvf function for running tests

    - set up the working directory
    - generate appropriate libs (neuron, coreneuron etc.)
    - call _single_run: once for each run_name

    Args:
        "cvf_stdrun -h" for args info

    Returns:
        out (dict): One RunResult for each run_name, protocol, mod file combination
    """

    logging.getLogger().setLevel(logging.INFO)

    # clear the state
    if clear_working_dir:
        utils.silent_remove([base_working_dir])

    config_files_dir = os.path.abspath(config_files_dir)

    run_config_path = os.path.abspath(run_config_path)
    with open(run_config_path, "r") as file:
        global_run_config = yaml.load(file, Loader=yaml.FullLoader)

    # Split run config
    global_modignore = global_run_config.pop("modignore")

    out = {}
    for run_name, run_config in global_run_config.items():
        modignore = copy.deepcopy(global_modignore)



        utils.nonoverriding_merge(modignore, run_config.get("modignore", {}))

        # prepare working dir
        working_dir = "{}/{}".format(base_working_dir, run_name)
        copy_to_working_dir_log = {}
        if clear_working_dir:
            copy_to_working_dir_log, _ = utils.init_working_dir(
                mod_dirs,
                working_dir,
                modignore.get("nocompile", {}),
            )

        # compile mechanisms
        for cmd in run_config["compile_commands"]:
            subprocess.run(cmd, cwd=working_dir, shell=True, env=os.environ)

        out[run_name] = _single_run(
            run_name,
            config_files_dir,
            config_protocol_generator,
            modignore.get("notest", {}),
            working_dir,
            print_config,
        )

        out[run_name].update(
            {
                modignore.get("nocompile", {})[Path(name).stem]: RunResult(
                    result=Result.SKIP,
                    modfile=Path(name).stem,
                    result_msg=modignore.get("nocompile", {})[Path(name).stem],
                )
                for name, is_copied in copy_to_working_dir_log.items()
                if not is_copied
            }
        )

    return out


def _single_run(
    run_name,
    config_files_dir,
    config_protocol_generator,
    run_config,
    working_dir,
    print_config,
):
    """Run tests with a particular run setup

    Args:
        run_name (str): run setup to be used
        config_files_dir (str): dir in which there are all the relevant config files
        config_protocol_generator (enum): policy for deciding inputs and protocols
            in case config is autogenerated
        run_config_path (str): run_config.yaml file location
        base_working_dir (str): working dir (it should be empty or non existant)
        print_config (bool): print config to yaml file

    Returns:
        out (dict): One RunResult for each protocol, mod file combination
    """
    results = {}
    modpaths = glob.glob(os.path.join(working_dir, "mod", "*.mod"))
    for modpath in modpaths:
        name = Path(modpath).stem
        if name not in run_config:
            config = Config(
                config_files_dir, modpath, config_protocol_generator, print_config
            )
            sim = Simulation(
                working_dir,
                config,
            )
            results.update(sim.run_all_protocols(run_name))
        else:
            modfile = run_config[name]
            results[modfile] = RunResult(
                result=Result.SKIP,
                modfile=name,
                result_msg=modfile,
            )

    return results


def compare(
    results,
    main_run="neuron",
    run_config_path="run_config.yaml",
    verbose=2,
    is_fail_on_error=True,
    output_dir="tmp"
):
    """Compare RunResult made by different runs

    Comparisons based on numpy.assert_allclose (https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_allclose.html)

    The comparison result format:

        CVF - <result> - <modfile>, <protocol>, max mse=XXX
                       - Traces: trace_name: max mse, ...

        trace name is composed as: <section_name>_<trace_name>[_in for inputs]

    Note: we suppose that the main run has at least all the run results present
    in the investigated run

    Example:

        CVF - SUCCESS - Ca_HVA, wiggle, max mse=1.0e-37
                      - Traces: soma_ica: ~1.e-36, soma_v_in: 0.0

    Args:
        results (dict): run results data.
        main_run (enum): results from this run are the base for comparison.
        run_config_path (str): we retrive tolerances specific for a particular run provided as dict: {run_name: (rtol, atol)}
        verbose (int): verbosity level
        is_fail_on_error (bool): stop program if comparison fail
        output_dir (str): dir where to save the comparison graphs
    """

    run_config_path = os.path.abspath(run_config_path)
    with open(run_config_path, "r") as file:
        global_run_config = yaml.load(file, Loader=yaml.FullLoader)

    logging.getLogger().setLevel(logging.INFO)
    is_error = False

    if len(results) < 2:
        logging.warning("Not enough runs to compare (<2).")
        return

    remove_duplicate_log = set()
    for run_name, tests in results.items():
        if run_name == main_run:
            continue

        logging.info("")
        logging.info("Compare {} with {}".format(main_run, run_name))

        atol = float(global_run_config[run_name].get("atol", 1e-8))
        rtol = float(global_run_config[run_name].get("rtol", 1e-5))
        # for base_res, test_res in zip(results[main_run], tests):
        for modfile, test_res in tests.items():
            if test_res.result is Result.SUCCESS:
                base_res = results[main_run][modfile]

                key = base_res.modfile + base_res.protocol + run_name
                test_res.mse = [
                    utils.compute_mse(base_trace, test_res.traces[trace_name])
                    for trace_name, base_trace in base_res.traces.items()
                ]

                try:
                    for trace_name, base_trace in base_res.traces.items():
                        test_trace = test_res.traces[trace_name]
                        np.testing.assert_allclose(base_trace, test_trace, rtol, atol)
                except AssertionError as e:
                    from matplotlib import pyplot as plt
                    plt.title("{} {} {} {} vs NEURON".format(base_res.modfile, base_res.protocol, run_name, trace_name))
                    plt.plot(test_res.tvec, test_trace, label=run_name)
                    plt.plot(test_res.tvec, base_trace, label="NEURON")
                    plt.xlabel("Time (s)")
                    plt.ylabel("Scale")
                    plt.legend()
                    plt.savefig("{}/{}_{}_{}_{}.pdf".format(output_dir, base_res.modfile, base_res.protocol, run_name, trace_name))
                    plt.clf()
                    test_res.result = Result.FAIL
                    test_res.result_msg = str(e)
                    is_error = True

            if verbose == 3 or (
                verbose == 2
                and (
                    test_res.result is not Result.SUCCESS
                    or key not in remove_duplicate_log
                )
            ):
                print(test_res)

            if test_res.result is Result.SUCCESS:
                remove_duplicate_log.add(key)

    if is_error and is_fail_on_error:
        raise CompareTestResultsError("Some tests failed")
    elif verbose == 1:
        logging.info("SUCCESS!")


def plot(results, dir=None):
    """Plot results wrapper to erase log10 warnings"""
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        _plot(results, dir)


def _plot(results, dir=None):
    """Plot results

    Plot results, one for each run_name, protocol, trace.
    Color-grouped by run_name.

    2 additional cumulative plots of all the traces: linear and log10

    Args:
        results (dict): run results data.
    """

    def plot_switcher(
        ifig, t, v, col, label, is_spiketrain, is_log=False, figname=None
    ):
        if len(v) == 0:
            return

        pplt.figure(ifig)
        if is_spiketrain:
            pplt.stem(
                v, [1] * len(v), linefmt=col, label=label, use_line_collection=True
            )
        elif is_log:
            pplt.plot(
                t,
                np.log10(abs(v)),
                color=col,
                label=label,
            )
        else:
            pplt.plot(
                t,
                v,
                color=col,
                label=label,
            )

        pplt.xlabel("t (msec)")
        pplt.legend()
        if figname:
            pplt.savefig(figname)

    logging.getLogger().setLevel(logging.INFO)
    if dir:
        utils.silent_remove([dir])
        os.makedirs(dir)

    remove_duplicate_log = set()

    colit = cycle(dict(colors.BASE_COLORS))

    i_fig = 1
    for (run_name, tests), col in zip(results.items(), dict(colors.BASE_COLORS)):
        if len(results) > 1:
            col = next(colit)
        for test in tests.values():

            for trace_name, trace_vec in test.traces.items():

                if len(results) == 1:
                    col = next(colit)

                is_spiketrain = "netcon" in trace_name

                label = "{}, {}, {}, {}".format(
                    test.modfile, test.protocol, test.run_name, trace_name
                )

                no_double_label = (label, "")[label in remove_duplicate_log]
                remove_duplicate_log.add(label)

                figname = (None, f"{dir}/fig_{i_fig}.png")[dir is not None]

                plot_switcher(
                    0,
                    test.tvec,
                    trace_vec,
                    col,
                    no_double_label,
                    is_spiketrain,
                    figname=figname,
                )

                plot_switcher(
                    1,
                    test.tvec,
                    trace_vec,
                    col,
                    no_double_label,
                    is_spiketrain,
                    is_log=True,
                    figname=figname,
                )

                i_fig += 1
                if i_fig > 20:
                    logging.warning("Too many figures! No visualization!")
                    if not dir:
                        return

                plot_switcher(
                    i_fig,
                    test.tvec,
                    trace_vec,
                    col,
                    label,
                    is_spiketrain,
                    figname=figname,
                )

        pplt.figure(0)
        pplt.xlabel("t (msec)")
        pplt.legend()
        pplt.title("cumulative")
        if dir:
            pplt.savefig(f"{dir}/fig_0.png")
        pplt.figure(1)
        pplt.xlabel("t (msec)")
        pplt.legend()
        pplt.title("cumulative")
        pplt.ylabel("log10(|y|)")
        if dir:
            pplt.savefig(f"{dir}/fig_1.png")

    if i_fig <= 20:
        pplt.show()


def get_conf(
    config_files_dir="config",
    mod_dirs=["mod/cvf", "mod/local"],
    working_dir="tmp",
    protocol_generator=Config.ProtocolGenerator.SI_FIRST_PROTOCOL,
):
    """Get config object (debugging)

    Does all the steps of run up to the config generation. After, it returns the object.

    Args:
        "cvf_stdrun -h" for args info

    Returns:
        conf (config): config object.
    """

    utils.init_working_dir(mod_dirs, working_dir)
    modpaths = glob.glob(os.path.join(working_dir, "mod", "*.mod"))

    conf = []
    for modpath in modpaths:
        if modpath.find("cvf") == -1:
            conf.append(Config(config_files_dir, modpath, protocol_generator))

    return conf


def cvf_print(results):
    """Print result data in yaml style

    Args:
        RunResult data
    """
    print(yaml.dump(utils.yamlfy(results), Dumper=utils.NoAliasDumper))
